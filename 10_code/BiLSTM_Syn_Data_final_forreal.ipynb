{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "colab": {
      "name": "BiLSTM_Syn_Data_final_forreal.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.9.0 64-bit ('venv1': conda)"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.0",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "accelerator": "GPU",
    "interpreter": {
      "hash": "7a6d299408f2023e0a200d93f96af30feb246c6cc222adc42d2612aa5ba88688"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Importing Required Packages"
      ],
      "metadata": {
        "id": "sd325WWli2PF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "import string\n",
        "import time\n",
        "import spacy\n",
        "import random\n",
        "from pathlib import Path\n",
        "import torch \n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchtext import data \n",
        "import torchtext\n",
        "from nltk.tokenize.treebank import TreebankWordDetokenizer"
      ],
      "outputs": [],
      "metadata": {
        "id": "gIrofHBUWn-P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset Preparation"
      ],
      "metadata": {
        "id": "jrX3a-jLi8LI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "source": [
        "# Read in data into a dataframe\n",
        "\n",
        "'''\n",
        "This data is a sample of 50000 rows from the original dataset of 1.6 million rows.\n",
        "'''\n",
        "\n",
        "df= pd.read_csv(\"/content/Synthetic_from_NB.csv\",index_col=None)\n",
        "df.columns=['labels','text']\n",
        "df.head()"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   labels                                               text\n",
              "0       1  helping sorry hope awesome band knicks place l...\n",
              "1       0  make online hehehe banned work canreally call ...\n",
              "2       0     one lost find pages well thanks messing around\n",
              "3       0  busy breakfast done bars hit good luck hiring ...\n",
              "4       0  itso hot gotta pi law head work oh nohope twit..."
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>labels</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>helping sorry hope awesome band knicks place l...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>make online hehehe banned work canreally call ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>one lost find pages well thanks messing around</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>busy breakfast done bars hit good luck hiring ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>itso hot gotta pi law head work oh nohope twit...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "_3JopK_zi4wu",
        "outputId": "79028ef8-cfa3-497f-c5df-c73b6e6f26bd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "source": [
        "# Cleaning data\n",
        "\n",
        "df.text= df.text.str.lower() #to lower-case\n",
        "\n",
        "punc=string.punctuation\n",
        "def cleaning_punctuations(text): #removing punctuation\n",
        "  translator=str.maketrans(\"\",\"\",punc)\n",
        "  return text.translate(translator)\n",
        "\n",
        "def pre_processing_01(tweet):\n",
        "  '''\n",
        "  Basic pre-processing to clean data. Removing:\n",
        "  1. Usernames\n",
        "  2. URLs\n",
        "  3. Special Characters\n",
        "  4. Multiple Spaces\n",
        "  5. Emails\n",
        "  6. Numbers\n",
        "  7. Single Chars\n",
        "  '''\n",
        "  tweet = tweet.apply(lambda x:re.sub('@[^\\s]+','',str(x))) # Remove Handles (aka usernames)\n",
        "  tweet = tweet.apply(lambda x:re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','',str(x))) # Remove URLs\n",
        "  tweet= tweet.apply(lambda x:' '.join(re.findall(r'\\w+', str(x)))) #remove special chars\n",
        "  tweet= tweet.apply(lambda x:cleaning_punctuations(x)) #remove punctuations\n",
        "  tweet = tweet.apply(lambda x:re.sub('@[^\\s]+','',str(x))) # Remove emails\n",
        "  tweet = tweet.apply(lambda x:re.sub('[0-9]+','',str(x))) # Remove numbers\n",
        "  tweet = tweet.apply(lambda x:re.sub(r'\\s+[a-zA-Z]\\s+', '', x))\n",
        "  return tweet\n",
        "\n",
        "df.text= pre_processing_01(df.text)\n",
        "df.text.head()\n"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    helping sorry hope awesome band knicks place l...\n",
              "1    make online hehehe banned work canreally call ...\n",
              "2       one lost find pages well thanks messing around\n",
              "3    busy breakfast done bars hit good luck hiring ...\n",
              "4    itso hot gotta pi law head work oh nohope twit...\n",
              "Name: text, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "618vYerDjupr",
        "outputId": "abca5ec4-ad65-46ee-8f00-81ab1c49bb81"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset Preparation: Pre-processing and Tokenization"
      ],
      "metadata": {
        "id": "jfU1m2n1kmtZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "source": [
        "# Declare fields for tweets and labels\n",
        "import torchtext\n",
        "from torchtext.legacy import data\n",
        "\n",
        "#Tokenization\n",
        "TEXT = data.Field(tokenize='spacy', lower=True, include_lengths= True) # include_lengths tells the RNN how long the actual sequences are\n",
        "LABEL = data.LabelField(dtype=torch.float)\n",
        "\n",
        "# Map data to fields\n",
        "fields = [('label', LABEL), ('text', TEXT)]\n",
        "\n",
        "# Apply field definition to create torch dataset\n",
        "dataset = torchtext.legacy.data.TabularDataset(\n",
        "        path=\"/content/Synthetic_from_NB.csv\",\n",
        "        format=\"CSV\",\n",
        "        fields=fields,\n",
        "        skip_header=False)\n",
        "\n",
        "#Split data into train, test, validation sets\n",
        "(train_data, test_data, valid_data) = dataset.split(split_ratio=[0.8,0.1,0.1])\n",
        "\n",
        "print(\"Number of train data: {}\".format(len(train_data)))\n",
        "print(\"Number of test data: {}\".format(len(test_data)))\n",
        "print(\"Number of validation data: {}\".format(len(valid_data)))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of train data: 1601\n",
            "Number of test data: 200\n",
            "Number of validation data: 200\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IEiVsHUekgBT",
        "outputId": "c9d8c127-f0c9-4dc5-db68-cbfb9acb3dba"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "source": [
        "print(vars(train_data.examples[0])) #visualizing train data\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'label': '0', 'text': ['one', 'today', 'reason', 'things', 'quiet', 'minimum', 'lake', 'twitter']}\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hoA43UJqkxS4",
        "outputId": "4497ab45-0e91-49c9-c3fe-6b1d1b4ebe3b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Pre-processing: Embedding using pre-trained GloVe embeddings.\n"
      ],
      "metadata": {
        "id": "U9oLSdw9k9CN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "source": [
        "MAX_VOCAB_SIZE = 25000\n",
        "\n",
        "# unk_init initializes words in the vocab using the Gaussian distribution\n",
        "TEXT.build_vocab(train_data, \n",
        "                 max_size = MAX_VOCAB_SIZE,\n",
        "                 vectors = \"glove.6B.100d\",\n",
        "                 unk_init = torch.Tensor.normal_)\n",
        "\n",
        "# embedding\n",
        "LABEL.build_vocab(train_data)\n",
        "\n",
        "# Printing the most frequent tokens\n",
        "TEXT.vocab.freqs.most_common(10)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('day', 152),\n",
              " ('work', 130),\n",
              " ('good', 129),\n",
              " ('go', 116),\n",
              " ('going', 110),\n",
              " ('today', 100),\n",
              " ('quot', 99),\n",
              " ('time', 97),\n",
              " ('get', 97),\n",
              " ('like', 87)]"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KNjN2U22k3S6",
        "outputId": "e2084a9b-aa39-4f93-8724-5350f9f7b341"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset Preparation: Sequencing and Padding using BucketIterator"
      ],
      "metadata": {
        "id": "tzNPXIjllOI-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "source": [
        "BATCH_SIZE = 128\n",
        "\n",
        "# sort_within_batch sorts all the tensors within a batch by their lengths\n",
        "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
        "    (train_data, valid_data, test_data),\n",
        "    batch_size = BATCH_SIZE,\n",
        "    sort_key = lambda x: len(x.text),\n",
        "    sort_within_batch = True)"
      ],
      "outputs": [],
      "metadata": {
        "id": "JZc_Gh4ulVa0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modelling"
      ],
      "metadata": {
        "id": "Bre09SSFlsaQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "source": [
        "class LSTM(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, \n",
        "                 bidirectional, dropout, pad_idx):\n",
        "        \"\"\"\n",
        "        Define the layers of the module.\n",
        "\n",
        "        vocab_size - vocabulary size\n",
        "        embedding_dim - size of the dense word vectors\n",
        "        hidden_dim - size of the hidden states\n",
        "        output_dim - number of classes\n",
        "        n_layers - number of multi-layer RNN\n",
        "        bidirectional - boolean - use both directions of LSTM\n",
        "        dropout - dropout probability\n",
        "        pad_idx -  string representing the pad token\n",
        "        \"\"\"\n",
        "        \n",
        "        super().__init__()\n",
        "\n",
        "        # 1. Feed the tweets in the embedding layer\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = pad_idx)\n",
        "\n",
        "        # 2. LSTM layer: returns the output and a tuple of the final hidden state and final cell state\n",
        "        self.encoder = nn.LSTM(embedding_dim, \n",
        "                               hidden_dim, \n",
        "                               num_layers=n_layers,\n",
        "                               bidirectional=bidirectional,\n",
        "                               dropout=dropout)\n",
        "        \n",
        "        # 3. Fully-connected layer: Final hidden state has both a forward and a backward component concatenated together\n",
        "        # The size of the input to the nn.Linear layer is twice that of the hidden dimension size\n",
        "        self.predictor = nn.Linear(hidden_dim*2, output_dim)\n",
        "\n",
        "        # Initialize dropout layer for regularization\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "      \n",
        "    def forward(self, text, text_lengths):\n",
        "        \"\"\"\n",
        "        The forward method is called when data is fed into the model.\n",
        "\n",
        "        text - [tweet length, batch size]\n",
        "        text_lengths - lengths of tweet\n",
        "        \"\"\"\n",
        "\n",
        "        # embedded = [sentence len, batch size, emb dim]\n",
        "        embedded = self.dropout(self.embedding(text))    \n",
        "        #By packing the embeddings, we cause RNN to only process non-padded elements. This speeds up computation\n",
        "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths)\n",
        "\n",
        "        # output of encoder\n",
        "        packed_output, (hidden, cell) = self.encoder(packed_embedded)\n",
        "\n",
        "        # unpack sequence - transform packed sequence to a tensor\n",
        "        output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output)\n",
        "\n",
        "        # output = [sentence len, batch size, hid dim * num directions]\n",
        "        # output over padding tokens are zero tensors\n",
        "        \n",
        "        # hidden = [num layers * num directions, batch size, hid dim]\n",
        "        # cell = [num layers * num directions, batch size, hid dim]\n",
        "        \n",
        "        # Get the final layer forward and backward hidden states  \n",
        "        # concat the final forward and backward hidden layers and apply dropout\n",
        "        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))\n",
        "\n",
        "        # hidden = [batch size, hid dim * num directions]\n",
        "\n",
        "        return self.predictor(hidden)\n",
        "\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "eTdYx8mUlIl3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create Model"
      ],
      "metadata": {
        "id": "9zjYCQFemb6Z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "source": [
        "INPUT_DIM = len(TEXT.vocab) # dim is equal to the dim of pre-trained GloVe vectors\n",
        "EMBEDDING_DIM = 100\n",
        "HIDDEN_DIM = 256\n",
        "OUTPUT_DIM = 1\n",
        "N_LAYERS = 2 # 2 layers of biLSTM\n",
        "BIDIRECTIONAL = True #Bi-directional LSTM\n",
        "DROPOUT = 0.5 # Dropout probability\n",
        "\n",
        "# Get pad token index from vocab\n",
        "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
        "\n",
        "# Create an instance of LSTM class\n",
        "model = LSTM(INPUT_DIM,\n",
        "            EMBEDDING_DIM,\n",
        "            HIDDEN_DIM,\n",
        "            OUTPUT_DIM,\n",
        "            N_LAYERS,\n",
        "            BIDIRECTIONAL,\n",
        "            DROPOUT,\n",
        "            PAD_IDX)"
      ],
      "outputs": [],
      "metadata": {
        "id": "4wEHtQAZmZVk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "source": [
        "# Copy the pre-trained word embeddings into the embedding layer\n",
        "pretrained_embeddings = TEXT.vocab.vectors\n",
        "\n",
        "# [vocab size, embedding dim]\n",
        "print(pretrained_embeddings.shape)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4560, 100])\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1lofbOonmnit",
        "outputId": "3c589639-23a2-4c99-e347-7ba9ee4df48c"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "source": [
        "# Replace the initial weights of the embedding layer with the pre-trained embeddings\n",
        "model.embedding.weight.data.copy_(pretrained_embeddings)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.1678, -0.0336,  0.3413,  ...,  0.2604, -0.8232,  0.2224],\n",
              "        [-0.2053, -0.7776,  0.7440,  ..., -0.0724, -0.7689, -0.2080],\n",
              "        [-0.3669,  0.4154,  0.1348,  ...,  0.0244,  0.2211,  0.4317],\n",
              "        ...,\n",
              "        [ 0.1238,  0.0467,  0.1646,  ..., -0.1151,  0.2209, -0.4480],\n",
              "        [-0.2372, -0.3621,  0.8482,  ...,  0.6932,  1.7314,  1.0317],\n",
              "        [ 0.0804, -0.0131, -0.3026,  ..., -0.0917, -0.5893, -0.0245]])"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QuV0Nv0amqjg",
        "outputId": "4531557a-1329-495d-b174-49318e3bccb9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "source": [
        "# Initialize <unk> and <pad> both to all zeros - irrelevant for sentiment analysis\n",
        "UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n",
        "\n",
        "# Setting row in the embedding weights matrix to zero using the token index\n",
        "model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\n",
        "model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)\n",
        "\n",
        "print(model.embedding.weight.data)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        [-0.3669,  0.4154,  0.1348,  ...,  0.0244,  0.2211,  0.4317],\n",
            "        ...,\n",
            "        [ 0.1238,  0.0467,  0.1646,  ..., -0.1151,  0.2209, -0.4480],\n",
            "        [-0.2372, -0.3621,  0.8482,  ...,  0.6932,  1.7314,  1.0317],\n",
            "        [ 0.0804, -0.0131, -0.3026,  ..., -0.0917, -0.5893, -0.0245]])\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fPcHZ8tlms3V",
        "outputId": "162d681a-a724-4a43-f410-262a4c4b8039"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Training"
      ],
      "metadata": {
        "id": "ikTh3WOAmwkq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "source": [
        "# Adam optimizer used to update the weights. We specify learning rate as 0.002\n",
        "optimizer = optim.Adam(model.parameters(), lr=2e-2)\n",
        "\n",
        "# Loss function: binary cross entropy with logits\n",
        "# It restricts the predictions to a number between 0 and 1 using the logit function\n",
        "criterion = nn.BCEWithLogitsLoss()"
      ],
      "outputs": [],
      "metadata": {
        "id": "Km2qBjspmuIo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "source": [
        "# Helper functions\n",
        "\n",
        "def batch_accuracy(predictions, label):\n",
        "    \"\"\"\n",
        "    Returns accuracy per batch.\n",
        "\n",
        "    predictions - float\n",
        "    label - 0 or 1\n",
        "    \"\"\"\n",
        "\n",
        "    # Round predictions to the closest integer using the sigmoid function\n",
        "    preds = torch.round(torch.sigmoid(predictions))\n",
        "    # If prediction is equal to label\n",
        "    correct = (preds == label).float()\n",
        "    # Average correct predictions\n",
        "    accuracy = correct.sum() / len(correct)\n",
        "\n",
        "    return accuracy\n",
        "\n",
        "def timer(start_time, end_time):\n",
        "    \"\"\"\n",
        "    Returns the minutes and seconds.\n",
        "    \"\"\"\n",
        "\n",
        "    time = end_time - start_time\n",
        "    mins = int(time / 60)\n",
        "    secs = int(time - (mins * 60))\n",
        "\n",
        "    return mins, secs"
      ],
      "outputs": [],
      "metadata": {
        "id": "uaVOMzOKm6yh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "source": [
        "def train(model, iterator, optimizer, criterion):\n",
        "    \"\"\"\n",
        "    Function to evaluate training loss and accuracy.\n",
        "\n",
        "    iterator - train iterator\n",
        "    \"\"\"\n",
        "    \n",
        "    # Cumulated Training loss\n",
        "    training_loss = 0.0\n",
        "    # Cumulated Training accuracy\n",
        "    training_acc = 0.0\n",
        "    \n",
        "    # Set model to training mode\n",
        "    model.train()\n",
        "    \n",
        "    # For each batch in the training iterator\n",
        "    for batch in iterator:\n",
        "        \n",
        "        # 1. Zero the gradients\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # batch.text is a tuple (tensor, len of seq)\n",
        "        text, text_lengths = batch.text\n",
        "        \n",
        "        # 2. Compute the predictions\n",
        "        predictions = model(text, text_lengths).squeeze(1)\n",
        "        \n",
        "        # 3. Compute loss\n",
        "        loss = criterion(predictions, batch.label)\n",
        "        \n",
        "        # Compute accuracy\n",
        "        accuracy = batch_accuracy(predictions, batch.label)\n",
        "        \n",
        "        # 4. Use loss to compute gradients\n",
        "        loss.backward()\n",
        "        \n",
        "        # 5. Use optimizer to take gradient step\n",
        "        optimizer.step()\n",
        "        \n",
        "        training_loss += loss.item()\n",
        "        training_acc += accuracy.item()\n",
        "    \n",
        "    # Return the loss and accuracy, averaged across each epoch\n",
        "    # len of iterator = num of batches in the iterator\n",
        "    return training_loss / len(iterator), training_acc / len(iterator)"
      ],
      "outputs": [],
      "metadata": {
        "id": "xZ3iCo-cm8Ta"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "    \"\"\"\n",
        "    Function to evaluate the loss and accuracy of validation and test sets.\n",
        "\n",
        "    iterator - validation or test iterator\n",
        "    \"\"\"\n",
        "    \n",
        "    # Cumulated Training loss\n",
        "    eval_loss = 0.0\n",
        "    # Cumulated Training accuracy\n",
        "    eval_acc = 0\n",
        "    \n",
        "    # Set model to evaluation mode\n",
        "    model.eval()\n",
        "    \n",
        "    # Don't calculate the gradients\n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for batch in iterator:\n",
        "\n",
        "            text, text_lengths = batch.text\n",
        "            \n",
        "            predictions = model(text, text_lengths).squeeze(1)\n",
        "            \n",
        "            loss = criterion(predictions, batch.label)\n",
        "            \n",
        "            accuracy = batch_accuracy(predictions, batch.label)\n",
        "\n",
        "            eval_loss += loss.item()\n",
        "            eval_acc += accuracy.item()\n",
        "        \n",
        "    return eval_loss / len(iterator), eval_acc / len(iterator)"
      ],
      "outputs": [],
      "metadata": {
        "id": "pGtogF9QnAIU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "source": [
        "# Number of epochs\n",
        "NUM_EPOCHS = 6\n",
        "\n",
        "# Lowest validation lost\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "\n",
        "    start_time = time.time()\n",
        "    \n",
        "    # Evaluate training loss and accuracy\n",
        "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
        "    # Evaluate validation loss and accuracy\n",
        "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
        "    \n",
        "    end_time = time.time()\n",
        "\n",
        "    mins, secs = timer(start_time, end_time)\n",
        "    \n",
        "    # At each epoch, if the validation loss is the best\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        # Save the parameters of the model\n",
        "        torch.save(model.state_dict(), 'model-small_syn.pt')\n",
        "\n",
        "    print(\"Epoch {}:\".format(epoch+1))\n",
        "    print(\"\\t Total Time: {}m {}s\".format(mins, secs))\n",
        "    print(\"\\t Train Loss {} | Train Accuracy: {}%\".format(round(train_loss, 2), round(train_acc*100, 2)))\n",
        "    print(\"\\t Validation Loss {} | Validation Accuracy: {}%\".format(round(valid_loss, 2), round(valid_acc*100, 2)))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1:\n",
            "\t Total Time: 0m 5s\n",
            "\t Train Loss 0.7 | Train Accuracy: 52.6%\n",
            "\t Validation Loss 0.72 | Validation Accuracy: 45.53%\n",
            "Epoch 2:\n",
            "\t Total Time: 0m 5s\n",
            "\t Train Loss 0.67 | Train Accuracy: 57.68%\n",
            "\t Validation Loss 0.70 | Validation Accuracy: 48.78%\n",
            "Epoch 3:\n",
            "\t Total Time: 0m 5s\n",
            "\t Train Loss 0.61 | Train Accuracy: 67.29%\n",
            "\t Validation Loss 0.61 | Validation Accuracy: 52.56%\n",
            "Epoch 4:\n",
            "\t Total Time: 0m 5s\n",
            "\t Train Loss 0.5 | Train Accuracy: 76.83%\n",
            "\t Validation Loss 0.42 | Validation Accuracy: 51.87%\n",
            "Epoch 5:\n",
            "\t Total Time: 0m 5s\n",
            "\t Train Loss 0.42 | Train Accuracy: 80.13%\n",
            "\t Validation Loss 0.41 | Validation Accuracy: 53.73%\n",
            "Epoch 6:\n",
            "\t Total Time: 0m 5s\n",
            "\t Train Loss 0.33 | Train Accuracy: 85.95%\n",
            "\t Validation Loss 0.41 | Validation Accuracy: 54.82%\n"
          ]
        }
      ],
      "metadata": {
        "id": "9Cvo-qbSnCe7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0eb68303-dfbf-481b-a1c6-5a00fc231b5c"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "source": [
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "# Load the model with the best validation loss\n",
        "model.load_state_dict(torch.load(\"/content/model-small_syn.pt\"))\n",
        "model.eval()"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LSTM(\n",
              "  (embedding): Embedding(4560, 100, padding_idx=1)\n",
              "  (encoder): LSTM(100, 256, num_layers=2, dropout=0.5, bidirectional=True)\n",
              "  (predictor): Linear(in_features=512, out_features=1, bias=True)\n",
              "  (dropout): Dropout(p=0.5, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w9rNvR0gxpu2",
        "outputId": "c0a71405-37ba-4b16-c9e8-8bb6804a706b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Predictions\n"
      ],
      "metadata": {
        "id": "kLqW4L18nbhd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "source": [
        "# Evaluate test loss and accuracy\n",
        "test_loss, test_acc = evaluate(model, test_iterator, criterion)\n",
        "\n",
        "print(\"Test Loss: {} | Test Acc: {}%\".format(round(0.32, 2), round(test_acc*100, 2)))\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 0.33 | Test Acc: 53.67%\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 358
        },
        "id": "KBc-UxLN0uQx",
        "outputId": "e84ab614-f3e1-4fe3-c601-ff1636eab11f"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "source": [
        "nlp = spacy.load('en')\n",
        "\n",
        "def predict(model, text, tokenized=True):\n",
        "    \"\"\"\n",
        "    Given a tweet, predict the sentiment.\n",
        "\n",
        "    text - a string or a a list of tokens\n",
        "    tokenized - True if text is a list of tokens, False if passing in a string\n",
        "    \"\"\"\n",
        "\n",
        "    # Sets the model to evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    if tokenized == False:\n",
        "        # Tokenizes the sentence\n",
        "        tokens = [token.text for token in nlp.tokenizer(text)]\n",
        "    else:\n",
        "        tokens = text\n",
        "\n",
        "    # Index the tokens by converting to the integer representation from the vocabulary\n",
        "    indexed_tokens = [TEXT.vocab.stoi[t] for t in tokens]\n",
        "    # Get the length of the text\n",
        "    length = [len(indexed_tokens)]\n",
        "    # Convert the indices to a tensor\n",
        "    tensor = torch.LongTensor(indexed_tokens).to(device)\n",
        "    # Add a batch dimension by unsqueezeing\n",
        "    tensor = tensor.unsqueeze(1)\n",
        "    # Converts the length into a tensor\n",
        "    length_tensor = torch.LongTensor(length)\n",
        "    # Convert prediction to be between 0 and 1 with the sigmoid function\n",
        "    prediction = torch.sigmoid(model(tensor, length_tensor))\n",
        "\n",
        "    # Return a single value from the prediction\n",
        "    return prediction.item()"
      ],
      "outputs": [],
      "metadata": {
        "id": "5HL6Z9vM77GG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "source": [
        "# device = torch.device('cpu' if torch.cuda.is_available() else 'cpu')\n",
        "# print('Using device:', device)\n",
        "# print()\n",
        "\n",
        "# if device.type == 'cpu':\n",
        "#     print(torch.cuda.get_device_name(0))\n",
        "#     print('Memory Usage:')\n",
        "#     print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
        "#     print('Cached:   ', round(torch.cuda.memory_cached(0)/1024**3,1), 'GB')\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n",
            "\n",
            "Tesla K80\n",
            "Memory Usage:\n",
            "Allocated: 0.0 GB\n",
            "Cached:    0.0 GB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/cuda/memory.py:386: FutureWarning: torch.cuda.memory_cached has been renamed to torch.cuda.memory_reserved\n",
            "  FutureWarning)\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d7FW4trb8Nnr",
        "outputId": "ecf65ee8-3ded-4af8-8de2-f577855dc2c7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "source": [
        "# Single example prediction from the test set\n",
        "print(\"Tweet: {}\".format(TreebankWordDetokenizer().detokenize(test_data[10].text)))\n",
        "print(\"Prediction: {}\".format(round(predict(model, test_data[10].text), 2)))\n",
        "print(\"True Label: {}\".format(test_data[10].label))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tweet: next weekend bummed thatmissed left building school going days late spent likegood\n",
            "Prediction: 0.4\n",
            "True Label: 0\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1xPRj-f48KFK",
        "outputId": "5834ed37-4541-4307-8f7a-607a22e77c50"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "source": [
        "# Example prediction from the test set\n",
        "\n",
        "# List to append data to\n",
        "d = []\n",
        "\n",
        "\n",
        "for idx in range(10):\n",
        "\n",
        "    # Detokenize the tweets from the test set\n",
        "    tweet = TreebankWordDetokenizer().detokenize(test_data[idx].text)\n",
        "                                                 \n",
        "    # Append tweet, prediction, and true label\n",
        "    d.append({'Tweet': tweet, 'Prediction': predict(model, test_data[idx].text), 'True Label': test_data[idx].label})\n",
        "\n",
        "# Convert list to dataframe\n",
        "pd.DataFrame(d)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                               Tweet  Prediction True Label\n",
              "0      going call miss lollove rock ca nt ugh are nt    0.476630          0\n",
              "1  performing gotta go solve nowve similar gotta ...    0.634752          0\n",
              "2   watch i ve next weekend called utter pieces sign    0.663592          1\n",
              "3  platform change plans wear end anymore worse t...    0.549170          0\n",
              "4  work forgot tweet beef kinda mood wit cooking ...    0.655245          1\n",
              "5  work could does nt quiet let know hehe keeps a...    0.473720          0\n",
              "6          work going well slow perth cost lace wear    0.580295          0\n",
              "7  voice hate money fun love well signed meeting ...    0.513391          1\n",
              "8      night ahead aw ready classs would know lt omg    0.467010          1\n",
              "9  learning genius pretty watch new best eating c...    0.581298          0"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Tweet</th>\n",
              "      <th>Prediction</th>\n",
              "      <th>True Label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>going call miss lollove rock ca nt ugh are nt</td>\n",
              "      <td>0.476630</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>performing gotta go solve nowve similar gotta ...</td>\n",
              "      <td>0.634752</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>watch i ve next weekend called utter pieces sign</td>\n",
              "      <td>0.663592</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>platform change plans wear end anymore worse t...</td>\n",
              "      <td>0.549170</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>work forgot tweet beef kinda mood wit cooking ...</td>\n",
              "      <td>0.655245</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>work could does nt quiet let know hehe keeps a...</td>\n",
              "      <td>0.473720</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>work going well slow perth cost lace wear</td>\n",
              "      <td>0.580295</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>voice hate money fun love well signed meeting ...</td>\n",
              "      <td>0.513391</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>night ahead aw ready classs would know lt omg</td>\n",
              "      <td>0.467010</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>learning genius pretty watch new best eating c...</td>\n",
              "      <td>0.581298</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "id": "R1sY72Ag8M6W",
        "outputId": "7892285f-8361-4242-c1df-05397a777b9b"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [],
      "outputs": [],
      "metadata": {
        "id": "92GSW5h4DUv3"
      }
    }
  ]
}