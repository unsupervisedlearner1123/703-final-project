{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BiLSTM - Real Data.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Importing Required Packages"
      ],
      "metadata": {
        "id": "sd325WWli2PF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "import string\n",
        "import time\n",
        "import spacy\n",
        "import random\n",
        "from pathlib import Path\n",
        "import torch \n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchtext import data \n",
        "import torchtext\n",
        "from nltk.tokenize.treebank import TreebankWordDetokenizer"
      ],
      "metadata": {
        "id": "gIrofHBUWn-P"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset Preparation"
      ],
      "metadata": {
        "id": "jrX3a-jLi8LI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read in data into a dataframe\n",
        "\n",
        "'''\n",
        "This data is a sample of 50000 rows from the original dataset of 1.6 million rows.\n",
        "'''\n",
        "\n",
        "df= pd.read_csv(\"/content/sentiment140-small.csv\",header=None,index_col=None,names=[\"label\",\"ID\",\"date\",\"query\",\"user\",\"text\"])\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "_3JopK_zi4wu",
        "outputId": "85b5a79f-7f80-4dd7-c6e0-d54bfd3f46b6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>ID</th>\n",
              "      <th>date</th>\n",
              "      <th>query</th>\n",
              "      <th>user</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1963243661</td>\n",
              "      <td>Fri May 29 12:25:40 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>pauliinainen</td>\n",
              "      <td>chillin' with my boyfriend this weekend</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1983325558</td>\n",
              "      <td>Sun May 31 12:56:32 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>maynaseric</td>\n",
              "      <td>@jshe going good.  thanks! || gloves too! keep...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>2203773645</td>\n",
              "      <td>Wed Jun 17 00:30:09 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>DianaZhang</td>\n",
              "      <td>SUCKS. SUCKS. SUCKS.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>2058740220</td>\n",
              "      <td>Sat Jun 06 15:32:14 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>DCBrent</td>\n",
              "      <td>@kayoungche I like to pretend bi boys are stra...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>1793363889</td>\n",
              "      <td>Thu May 14 02:24:30 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>Nicholas698</td>\n",
              "      <td>Got a blood test later for Gladular Fever!</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   label  ...                                               text\n",
              "0      1  ...           chillin' with my boyfriend this weekend \n",
              "1      1  ...  @jshe going good.  thanks! || gloves too! keep...\n",
              "2      0  ...                              SUCKS. SUCKS. SUCKS. \n",
              "3      1  ...  @kayoungche I like to pretend bi boys are stra...\n",
              "4      0  ...        Got a blood test later for Gladular Fever! \n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cleaning data\n",
        "\n",
        "df.text= df.text.str.lower() #to lower-case\n",
        "\n",
        "punc=string.punctuation\n",
        "def cleaning_punctuations(text): #removing punctuation\n",
        "  translator=str.maketrans(\"\",\"\",punc)\n",
        "  return text.translate(translator)\n",
        "\n",
        "def pre_processing_01(tweet):\n",
        "  '''\n",
        "  Basic pre-processing to clean data. Removing:\n",
        "  1. Usernames\n",
        "  2. URLs\n",
        "  3. Special Characters\n",
        "  4. Multiple Spaces\n",
        "  5. Emails\n",
        "  6. Numbers\n",
        "  7. Single Chars\n",
        "  '''\n",
        "  tweet = tweet.apply(lambda x:re.sub('@[^\\s]+','',str(x))) # Remove Handles (aka usernames)\n",
        "  tweet = tweet.apply(lambda x:re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','',str(x))) # Remove URLs\n",
        "  tweet= tweet.apply(lambda x:' '.join(re.findall(r'\\w+', str(x)))) #remove special chars\n",
        "  tweet= tweet.apply(lambda x:cleaning_punctuations(x)) #remove punctuations\n",
        "  tweet = tweet.apply(lambda x:re.sub('@[^\\s]+','',str(x))) # Remove emails\n",
        "  tweet = tweet.apply(lambda x:re.sub('[0-9]+','',str(x))) # Remove numbers\n",
        "  tweet = tweet.apply(lambda x:re.sub(r'\\s+[a-zA-Z]\\s+', '', x))\n",
        "  return tweet\n",
        "\n",
        "df.text= pre_processing_01(df.text)\n",
        "df.text.head()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "618vYerDjupr",
        "outputId": "5fd32b2f-649a-4ea5-a5d2-907f3b4787ed"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0               chillin with my boyfriend this weekend\n",
              "1    going good thanks gloves too keep hands warm g...\n",
              "2                                    sucks sucks sucks\n",
              "3    i like to pretend bi boys are straight andm ju...\n",
              "4               gotblood test later for gladular fever\n",
              "Name: text, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset Preparation: Pre-processing and Tokenization"
      ],
      "metadata": {
        "id": "jfU1m2n1kmtZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Declare fields for tweets and labels\n",
        "import torchtext\n",
        "from torchtext.legacy import data\n",
        "\n",
        "#Tokenization\n",
        "TEXT = data.Field(tokenize='spacy', lower=True, include_lengths= True) # include_lengths tells the RNN how long the actual sequences are\n",
        "LABEL = data.LabelField(dtype=torch.float)\n",
        "\n",
        "# Map data to fields\n",
        "fields = [('label', LABEL), ('id',None),('date',None),('query',None),\n",
        "      ('user',None), ('text', TEXT)]\n",
        "\n",
        "# Apply field definition to create torch dataset\n",
        "dataset = torchtext.legacy.data.TabularDataset(\n",
        "        path=\"/content/sentiment140-small.csv\",\n",
        "        format=\"CSV\",\n",
        "        fields=fields,\n",
        "        skip_header=False)\n",
        "\n",
        "#Split data into train, test, validation sets\n",
        "(train_data, test_data, valid_data) = dataset.split(split_ratio=[0.8,0.1,0.1])\n",
        "\n",
        "print(\"Number of train data: {}\".format(len(train_data)))\n",
        "print(\"Number of test data: {}\".format(len(test_data)))\n",
        "print(\"Number of validation data: {}\".format(len(valid_data)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IEiVsHUekgBT",
        "outputId": "b84103f2-7a84-41d1-957f-9d85b0221a2e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of train data: 40000\n",
            "Number of test data: 5000\n",
            "Number of validation data: 5000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(vars(train_data.examples[0])) #visualizing train data\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hoA43UJqkxS4",
        "outputId": "66d0372e-e109-42a9-89c8-ede7f6022590"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'label': '1', 'text': ['let', \"'s\", 'see', 'how', 'much', 'i', 'can', 'get', 'done', 'by', '9', 'pm', 'today', '.']}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Pre-processing: Embedding using pre-trained GloVe embeddings.\n"
      ],
      "metadata": {
        "id": "U9oLSdw9k9CN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_VOCAB_SIZE = 25000\n",
        "\n",
        "# unk_init initializes words in the vocab using the Gaussian distribution\n",
        "TEXT.build_vocab(train_data, \n",
        "                 max_size = MAX_VOCAB_SIZE,\n",
        "                 vectors = \"glove.6B.100d\",\n",
        "                 unk_init = torch.Tensor.normal_)\n",
        "\n",
        "# embedding\n",
        "LABEL.build_vocab(train_data)\n",
        "\n",
        "# Printing the most frequent tokens\n",
        "TEXT.vocab.freqs.most_common(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KNjN2U22k3S6",
        "outputId": "8c4e9965-a3d3-4237-de39-94dbbbb306ab"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('i', 25006),\n",
              " ('!', 22806),\n",
              " ('.', 20300),\n",
              " (' ', 14625),\n",
              " ('to', 14227),\n",
              " ('the', 13204),\n",
              " (',', 11827),\n",
              " ('a', 9548),\n",
              " ('my', 7894),\n",
              " ('and', 7642)]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset Preparation: Sequencing and Padding using BucketIterator"
      ],
      "metadata": {
        "id": "tzNPXIjllOI-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 128\n",
        "\n",
        "# sort_within_batch sorts all the tensors within a batch by their lengths\n",
        "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
        "    (train_data, valid_data, test_data),\n",
        "    batch_size = BATCH_SIZE,\n",
        "    sort_key = lambda x: len(x.text),\n",
        "    sort_within_batch = True)"
      ],
      "metadata": {
        "id": "JZc_Gh4ulVa0"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modelling"
      ],
      "metadata": {
        "id": "Bre09SSFlsaQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTM(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, \n",
        "                 bidirectional, dropout, pad_idx):\n",
        "        \"\"\"\n",
        "        Define the layers of the module.\n",
        "\n",
        "        vocab_size - vocabulary size\n",
        "        embedding_dim - size of the dense word vectors\n",
        "        hidden_dim - size of the hidden states\n",
        "        output_dim - number of classes\n",
        "        n_layers - number of multi-layer RNN\n",
        "        bidirectional - boolean - use both directions of LSTM\n",
        "        dropout - dropout probability\n",
        "        pad_idx -  string representing the pad token\n",
        "        \"\"\"\n",
        "        \n",
        "        super().__init__()\n",
        "\n",
        "        # 1. Feed the tweets in the embedding layer\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = pad_idx)\n",
        "\n",
        "        # 2. LSTM layer: returns the output and a tuple of the final hidden state and final cell state\n",
        "        self.encoder = nn.LSTM(embedding_dim, \n",
        "                               hidden_dim, \n",
        "                               num_layers=n_layers,\n",
        "                               bidirectional=bidirectional,\n",
        "                               dropout=dropout)\n",
        "        \n",
        "        # 3. Fully-connected layer: Final hidden state has both a forward and a backward component concatenated together\n",
        "        # The size of the input to the nn.Linear layer is twice that of the hidden dimension size\n",
        "        self.predictor = nn.Linear(hidden_dim*2, output_dim)\n",
        "\n",
        "        # Initialize dropout layer for regularization\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "      \n",
        "    def forward(self, text, text_lengths):\n",
        "        \"\"\"\n",
        "        The forward method is called when data is fed into the model.\n",
        "\n",
        "        text - [tweet length, batch size]\n",
        "        text_lengths - lengths of tweet\n",
        "        \"\"\"\n",
        "\n",
        "        # embedded = [sentence len, batch size, emb dim]\n",
        "        embedded = self.dropout(self.embedding(text))    \n",
        "        #By packing the embeddings, we cause RNN to only process non-padded elements. This speeds up computation\n",
        "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths)\n",
        "\n",
        "        # output of encoder\n",
        "        packed_output, (hidden, cell) = self.encoder(packed_embedded)\n",
        "\n",
        "        # unpack sequence - transform packed sequence to a tensor\n",
        "        output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output)\n",
        "\n",
        "        # output = [sentence len, batch size, hid dim * num directions]\n",
        "        # output over padding tokens are zero tensors\n",
        "        \n",
        "        # hidden = [num layers * num directions, batch size, hid dim]\n",
        "        # cell = [num layers * num directions, batch size, hid dim]\n",
        "        \n",
        "        # Get the final layer forward and backward hidden states  \n",
        "        # concat the final forward and backward hidden layers and apply dropout\n",
        "        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))\n",
        "\n",
        "        # hidden = [batch size, hid dim * num directions]\n",
        "\n",
        "        return self.predictor(hidden)\n",
        "\n"
      ],
      "metadata": {
        "id": "eTdYx8mUlIl3"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create Model"
      ],
      "metadata": {
        "id": "9zjYCQFemb6Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "INPUT_DIM = len(TEXT.vocab) # dim is equal to the dim of pre-trained GloVe vectors\n",
        "EMBEDDING_DIM = 100\n",
        "HIDDEN_DIM = 256\n",
        "OUTPUT_DIM = 1\n",
        "N_LAYERS = 2 # 2 layers of biLSTM\n",
        "BIDIRECTIONAL = True #Bi-directional LSTM\n",
        "DROPOUT = 0.5 # Dropout probability\n",
        "\n",
        "# Get pad token index from vocab\n",
        "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
        "\n",
        "# Create an instance of LSTM class\n",
        "model = LSTM(INPUT_DIM,\n",
        "            EMBEDDING_DIM,\n",
        "            HIDDEN_DIM,\n",
        "            OUTPUT_DIM,\n",
        "            N_LAYERS,\n",
        "            BIDIRECTIONAL,\n",
        "            DROPOUT,\n",
        "            PAD_IDX)"
      ],
      "metadata": {
        "id": "4wEHtQAZmZVk"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Copy the pre-trained word embeddings into the embedding layer\n",
        "pretrained_embeddings = TEXT.vocab.vectors\n",
        "\n",
        "# [vocab size, embedding dim]\n",
        "print(pretrained_embeddings.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1lofbOonmnit",
        "outputId": "5459edab-60d3-4f8b-c39f-4edfbe0c7129"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([25002, 100])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace the initial weights of the embedding layer with the pre-trained embeddings\n",
        "model.embedding.weight.data.copy_(pretrained_embeddings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QuV0Nv0amqjg",
        "outputId": "7aac7522-4eea-4b8c-fba2-d08529b3d1bd"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.4161, -0.5342, -0.4833,  ...,  1.0740, -0.3773,  0.1998],\n",
              "        [ 2.6477, -1.5362,  0.0863,  ...,  0.8388,  0.8975, -1.4126],\n",
              "        [-0.0465,  0.6197,  0.5665,  ..., -0.3762, -0.0325,  0.8062],\n",
              "        ...,\n",
              "        [-0.5065,  0.7538,  3.3818,  ...,  0.1182,  0.0792, -1.3691],\n",
              "        [ 0.8347, -0.7449, -0.0376,  ..., -1.8831,  2.6144,  0.2968],\n",
              "        [-1.2980, -0.6180,  1.2960,  ..., -0.0291,  3.0614, -0.6824]])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize <unk> and <pad> both to all zeros - irrelevant for sentiment analysis\n",
        "UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n",
        "\n",
        "# Setting row in the embedding weights matrix to zero using the token index\n",
        "model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\n",
        "model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)\n",
        "\n",
        "print(model.embedding.weight.data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fPcHZ8tlms3V",
        "outputId": "73a2ee85-ce50-4b5a-e518-382dc07b8928"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        [-0.0465,  0.6197,  0.5665,  ..., -0.3762, -0.0325,  0.8062],\n",
            "        ...,\n",
            "        [-0.5065,  0.7538,  3.3818,  ...,  0.1182,  0.0792, -1.3691],\n",
            "        [ 0.8347, -0.7449, -0.0376,  ..., -1.8831,  2.6144,  0.2968],\n",
            "        [-1.2980, -0.6180,  1.2960,  ..., -0.0291,  3.0614, -0.6824]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Training"
      ],
      "metadata": {
        "id": "ikTh3WOAmwkq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Adam optimizer used to update the weights. We specify learning rate as 0.002\n",
        "optimizer = optim.Adam(model.parameters(), lr=2e-2)\n",
        "\n",
        "# Loss function: binary cross entropy with logits\n",
        "# It restricts the predictions to a number between 0 and 1 using the logit function\n",
        "criterion = nn.BCEWithLogitsLoss()"
      ],
      "metadata": {
        "id": "Km2qBjspmuIo"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper functions\n",
        "\n",
        "def batch_accuracy(predictions, label):\n",
        "    \"\"\"\n",
        "    Returns accuracy per batch.\n",
        "\n",
        "    predictions - float\n",
        "    label - 0 or 1\n",
        "    \"\"\"\n",
        "\n",
        "    # Round predictions to the closest integer using the sigmoid function\n",
        "    preds = torch.round(torch.sigmoid(predictions))\n",
        "    # If prediction is equal to label\n",
        "    correct = (preds == label).float()\n",
        "    # Average correct predictions\n",
        "    accuracy = correct.sum() / len(correct)\n",
        "\n",
        "    return accuracy\n",
        "\n",
        "def timer(start_time, end_time):\n",
        "    \"\"\"\n",
        "    Returns the minutes and seconds.\n",
        "    \"\"\"\n",
        "\n",
        "    time = end_time - start_time\n",
        "    mins = int(time / 60)\n",
        "    secs = int(time - (mins * 60))\n",
        "\n",
        "    return mins, secs"
      ],
      "metadata": {
        "id": "uaVOMzOKm6yh"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, iterator, optimizer, criterion):\n",
        "    \"\"\"\n",
        "    Function to evaluate training loss and accuracy.\n",
        "\n",
        "    iterator - train iterator\n",
        "    \"\"\"\n",
        "    \n",
        "    # Cumulated Training loss\n",
        "    training_loss = 0.0\n",
        "    # Cumulated Training accuracy\n",
        "    training_acc = 0.0\n",
        "    \n",
        "    # Set model to training mode\n",
        "    model.train()\n",
        "    \n",
        "    # For each batch in the training iterator\n",
        "    for batch in iterator:\n",
        "        \n",
        "        # 1. Zero the gradients\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # batch.text is a tuple (tensor, len of seq)\n",
        "        text, text_lengths = batch.text\n",
        "        \n",
        "        # 2. Compute the predictions\n",
        "        predictions = model(text, text_lengths).squeeze(1)\n",
        "        \n",
        "        # 3. Compute loss\n",
        "        loss = criterion(predictions, batch.label)\n",
        "        \n",
        "        # Compute accuracy\n",
        "        accuracy = batch_accuracy(predictions, batch.label)\n",
        "        \n",
        "        # 4. Use loss to compute gradients\n",
        "        loss.backward()\n",
        "        \n",
        "        # 5. Use optimizer to take gradient step\n",
        "        optimizer.step()\n",
        "        \n",
        "        training_loss += loss.item()\n",
        "        training_acc += accuracy.item()\n",
        "    \n",
        "    # Return the loss and accuracy, averaged across each epoch\n",
        "    # len of iterator = num of batches in the iterator\n",
        "    return training_loss / len(iterator), training_acc / len(iterator)"
      ],
      "metadata": {
        "id": "xZ3iCo-cm8Ta"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "    \"\"\"\n",
        "    Function to evaluate the loss and accuracy of validation and test sets.\n",
        "\n",
        "    iterator - validation or test iterator\n",
        "    \"\"\"\n",
        "    \n",
        "    # Cumulated Training loss\n",
        "    eval_loss = 0.0\n",
        "    # Cumulated Training accuracy\n",
        "    eval_acc = 0\n",
        "    \n",
        "    # Set model to evaluation mode\n",
        "    model.eval()\n",
        "    \n",
        "    # Don't calculate the gradients\n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for batch in iterator:\n",
        "\n",
        "            text, text_lengths = batch.text\n",
        "            \n",
        "            predictions = model(text, text_lengths).squeeze(1)\n",
        "            \n",
        "            loss = criterion(predictions, batch.label)\n",
        "            \n",
        "            accuracy = batch_accuracy(predictions, batch.label)\n",
        "\n",
        "            eval_loss += loss.item()\n",
        "            eval_acc += accuracy.item()\n",
        "        \n",
        "    return eval_loss / len(iterator), eval_acc / len(iterator)"
      ],
      "metadata": {
        "id": "pGtogF9QnAIU"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of epochs\n",
        "NUM_EPOCHS = 6\n",
        "\n",
        "# Lowest validation lost\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "\n",
        "    start_time = time.time()\n",
        "    \n",
        "    # Evaluate training loss and accuracy\n",
        "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
        "    # Evaluate validation loss and accuracy\n",
        "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
        "    \n",
        "    end_time = time.time()\n",
        "\n",
        "    mins, secs = timer(start_time, end_time)\n",
        "    \n",
        "    # At each epoch, if the validation loss is the best\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        # Save the parameters of the model\n",
        "        torch.save(model.state_dict(), 'model-small.pt')\n",
        "\n",
        "    print(\"Epoch {}:\".format(epoch+1))\n",
        "    print(\"\\t Total Time: {}m {}s\".format(mins, secs))\n",
        "    print(\"\\t Train Loss {} | Train Accuracy: {}%\".format(round(train_loss, 2), round(train_acc*100, 2)))\n",
        "    print(\"\\t Validation Loss {} | Validation Accuracy: {}%\".format(round(valid_loss, 2), round(valid_acc*100, 2)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Cvo-qbSnCe7",
        "outputId": "b02d507c-cc26-44a2-ebde-a66820e3a425"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1:\n",
            "\t Total Time: 5m 33s\n",
            "\t Train Loss 0.7 | Train Accuracy: 56.29%\n",
            "\t Validation Loss 0.67 | Validation Accuracy: 58.85%\n",
            "Epoch 2:\n",
            "\t Total Time: 5m 48s\n",
            "\t Train Loss 0.62 | Train Accuracy: 66.25%\n",
            "\t Validation Loss 0.57 | Validation Accuracy: 71.25%\n",
            "Epoch 3:\n",
            "\t Total Time: 6m 20s\n",
            "\t Train Loss 0.54 | Train Accuracy: 73.31%\n",
            "\t Validation Loss 0.54 | Validation Accuracy: 72.4%\n",
            "Epoch 4:\n",
            "\t Total Time: 8m 49s\n",
            "\t Train Loss 0.58 | Train Accuracy: 70.47%\n",
            "\t Validation Loss 0.58 | Validation Accuracy: 69.57%\n",
            "Epoch 5:\n",
            "\t Total Time: 12m 15s\n",
            "\t Train Loss 0.6 | Train Accuracy: 68.23%\n",
            "\t Validation Loss 0.58 | Validation Accuracy: 69.94%\n",
            "Epoch 6:\n",
            "\t Total Time: 15m 51s\n",
            "\t Train Loss 0.61 | Train Accuracy: 68.1%\n",
            "\t Validation Loss 0.59 | Validation Accuracy: 67.97%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "# Load the model with the best validation loss\n",
        "model.load_state_dict(torch.load(\"/content/model-small.pt\"))\n",
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w9rNvR0gxpu2",
        "outputId": "1a4b1e61-dbab-4d36-e39f-21e4f95d909a"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LSTM(\n",
              "  (embedding): Embedding(25002, 100, padding_idx=1)\n",
              "  (encoder): LSTM(100, 256, num_layers=2, dropout=0.5, bidirectional=True)\n",
              "  (predictor): Linear(in_features=512, out_features=1, bias=True)\n",
              "  (dropout): Dropout(p=0.5, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Predictions\n"
      ],
      "metadata": {
        "id": "kLqW4L18nbhd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate test loss and accuracy\n",
        "test_loss, test_acc = evaluate(model, test_iterator, criterion)\n",
        "\n",
        "print(\"Test Loss: {} | Test Acc: {}%\".format(round(test_loss, 2), round(test_acc*100, 2)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KBc-UxLN0uQx",
        "outputId": "e0fbe990-7341-43e5-caad-34b6c9f90d4f"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 0.75 | Test Acc: 58.54%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "3YyIYZrX1-La"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}